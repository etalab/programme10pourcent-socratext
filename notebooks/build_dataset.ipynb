{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942bc5a-1b65-49d4-8295-c9cbdc9dd461",
   "metadata": {},
   "source": [
    "Loading annotations exported from Label Studio as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c10ddf-4529-433a-b69d-2dfb1dd1d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/sample/sample.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "263711e1-d2e8-401f-a9e9-da5b17fd4412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3039bcb6-86ac-45a2-afff-ba39778683a0",
   "metadata": {},
   "source": [
    "35 images were annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a991251-9441-4c9d-b4b3-9fa6b8d6dc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'annotations', 'predictions', 'file_upload', 'data', 'meta', 'created_at', 'updated_at', 'project'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12297bf-a491-4ba6-b63f-7c848e85a62c",
   "metadata": {},
   "source": [
    "An annotation for a single image is a dictionary. Everything which is interesting for us is in the `annotations` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e8b41e8-0c0f-4d93-aa3d-ff895442d827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'completed_by', 'result', 'was_cancelled', 'ground_truth', 'created_at', 'updated_at', 'lead_time', 'prediction', 'result_count', 'task'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['annotations'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76492dc4-aa33-4913-b887-6724c8ede1c4",
   "metadata": {},
   "source": [
    "And specifically in the `result` value inside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128e5ba-e4e8-4f70-9b01-ba3ca9a8a74b",
   "metadata": {},
   "source": [
    "Below functions to format annotations before implementing the `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "974f9ee7-f434-479d-a26b-cf310cac6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def format_image_annotations(image_annotations: Dict):\n",
    "    \"\"\"\n",
    "    Formats the annotations for a single image.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for annotation in image_annotations['result']:\n",
    "        value = annotation['value']\n",
    "        if 'rectanglelabels' not in value.keys():\n",
    "            continue\n",
    "        # x, y, width, height are already normalized and in 0-100\n",
    "        # For LayoutLMv2 we want them in 0-1000\n",
    "        x = value['x']\n",
    "        y = value['y']\n",
    "        width = value['width']\n",
    "        height = value['height']\n",
    "        rotation = value['rotation']\n",
    "\n",
    "        # ignoring the rotation parameter for now\n",
    "        #  [x1, y1, x3, y3] format\n",
    "        x1 = 10 * x\n",
    "        y1 = 10 * (100 - y - height)\n",
    "        x3 = 10 * (x + width)\n",
    "        y3 = 10 * (100 - y)\n",
    "\n",
    "        boxes.append([x1, y1, x3, y3])\n",
    "        words.append(annotation['meta']['text'][0])\n",
    "        labels.append(value['rectanglelabels'][0])\n",
    "    \n",
    "    return words, labels, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c1969360-de59-4467-857b-50162ae317f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def format_annotations(annotations: List):\n",
    "    \"\"\"\n",
    "    Format all annotations.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for image_full_annotations in annotations:\n",
    "        image_annotations = image_full_annotations['annotations'][0]\n",
    "        image_words, image_labels, image_boxes = format_image_annotations(image_annotations)\n",
    "        words.append(image_words)\n",
    "        boxes.append(image_boxes)\n",
    "        labels.append(image_labels)\n",
    "    \n",
    "    return words, labels, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f23913-6639-45df-860f-1c6104384fbf",
   "metadata": {},
   "source": [
    "We format all annotations using the `format_annotations` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2be3f4e8-961d-47bd-a58d-c81d783f07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_annotations = format_annotations(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe4f55-1840-4fed-b6cb-aa05855281ca",
   "metadata": {},
   "source": [
    "Now we implement the `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "649f0e52-2997-4317-917a-0638e7f36009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(annotations: List):\n",
    "    \"\"\"\n",
    "    Gets image paths from annotations.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for image_annotations in annotations:\n",
    "        image_paths.append(image_annotations['data']['image'])\n",
    "    return image_paths\n",
    "\n",
    "image_paths = get_image_paths(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "71ed171b-7e97-45eb-a022-0d491d45c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [path[21:] for path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d18c8e57-81c2-4633-b5e0-2efd3253e633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/sample/2022-02-02_repas_les jardins de presbourg.jpg',\n",
       " '../data/sample/2022-01-25_repas_la cantine de belleville.jpg',\n",
       " '../data/sample/2022-02-15_achat_la poste.jpg',\n",
       " '../data/sample/2022-03-28_repas_la comete.jpg',\n",
       " '../data/sample/20221108_230713.jpg',\n",
       " '../data/sample/20221019_091203.jpg',\n",
       " '../data/sample/20221030_123439.jpg',\n",
       " '../data/sample/2022-03-22_achat_la poste.jpg',\n",
       " '../data/sample/20221108_230510.jpg',\n",
       " '../data/sample/20221105_133703.jpg',\n",
       " '../data/sample/20221019_091208.jpg',\n",
       " '../data/sample/20221108_230524.jpg',\n",
       " '../data/sample/2022-03-10_achat_la poste.jpg',\n",
       " '../data/sample/IMG_20221107_211244.jpg',\n",
       " '../data/sample/2022-03-06_achat_darty.jpg',\n",
       " '../data/sample/20221108_230653.jpg',\n",
       " '../data/sample/20221030_123424.jpg',\n",
       " '../data/sample/2022-03-06_achat_le divan.jpg',\n",
       " '../data/sample/20221117_120342.jpg',\n",
       " '../data/sample/20221108_230535.jpg',\n",
       " '../data/sample/20221018_185931.jpg',\n",
       " '../data/sample/20221105_133551.jpg',\n",
       " '../data/sample/20221105_220557.jpg',\n",
       " '../data/sample/20221025_161620.jpg',\n",
       " '../data/sample/20221108_230625.jpg',\n",
       " '../data/sample/2022-03-11_repas_le floreal l abondance .jpg',\n",
       " '../data/sample/2022-02-17_repas_societe th grimmeisen atelier des melanges .jpg',\n",
       " '../data/sample/20221105_133631.jpg',\n",
       " '../data/sample/20221108_230546.jpg',\n",
       " '../data/sample/20221019_091245.jpg',\n",
       " '../data/sample/20221108_230608.jpg',\n",
       " '../data/sample/20221108_230634.jpg',\n",
       " '../data/sample/2022-03-30_repas_petit navire.jpg',\n",
       " '../data/sample/20221117_120322.jpg',\n",
       " '../data/sample/20221024_222905.jpg']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths = ['../data/sample/' + name for name in image_names]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de08a537-1776-48fd-b8c9-5f9044816196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': 0,\n",
       " 'total_price': 1,\n",
       " 'item_name': 2,\n",
       " 'item_total_price': 3,\n",
       " 'code_tva': 4,\n",
       " 'item_quantity': 5,\n",
       " 'item_unit_price': 6,\n",
       " 'taux_tva': 7,\n",
       " 'magasin': 8}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = [item for sublist in formatted_annotations[1] for item in sublist]\n",
    "labels = list(set(all_labels))\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a6d173d1-4238-4c6b-b476-dc5b3788097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class TicketsDataset(Dataset):\n",
    "    \"\"\"Tickets dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, annotations, image_paths: List[str], processor=None, max_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n",
    "            image_paths (string): Directory with all the document images.\n",
    "            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n",
    "        \"\"\"\n",
    "        self.words, self.labels, self.boxes = annotations\n",
    "        self.image_paths = image_paths\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # first, take an image\n",
    "        path = self.image_paths[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        # get word-level annotations \n",
    "        words = self.words[idx]\n",
    "        boxes = self.boxes[idx]\n",
    "        word_labels = self.labels[idx]\n",
    "\n",
    "        assert len(words) == len(boxes) == len(word_labels)\n",
    "\n",
    "        word_labels = [label2id[label] for label in word_labels]\n",
    "        # use processor to prepare everything\n",
    "        encoded_inputs = self.processor(image, words, boxes=boxes, word_labels=word_labels, \n",
    "                                        padding=\"max_length\", truncation=True, \n",
    "                                        return_tensors=\"pt\")\n",
    "        \n",
    "        # remove batch dimension\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k] = v.squeeze()\n",
    "\n",
    "        assert encoded_inputs.input_ids.shape == torch.Size([512])\n",
    "        assert encoded_inputs.attention_mask.shape == torch.Size([512])\n",
    "        assert encoded_inputs.token_type_ids.shape == torch.Size([512])\n",
    "        assert encoded_inputs.bbox.shape == torch.Size([512, 4])\n",
    "        assert encoded_inputs.image.shape == torch.Size([3, 224, 224])\n",
    "        assert encoded_inputs.labels.shape == torch.Size([512]) \n",
    "      \n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5290f4f-0e54-4262-a947-e58284a44ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/mamba/lib/python3.10/site-packages (4.24.0)\n",
      "Requirement already satisfied: filelock in /opt/mamba/lib/python3.10/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/mamba/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/mamba/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/mamba/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/mamba/lib/python3.10/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/mamba/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/mamba/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/mamba/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/mamba/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/mamba/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9b541122-50ed-4f01-ba7f-4f9f071e3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv2Processor\n",
    "\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n",
    "train_dataset = TicketsDataset(annotations=formatted_annotations,\n",
    "                            image_paths=image_paths, \n",
    "                            processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "80ebb987-0c16-43c2-88d4-34e9693ae2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TicketsDataset at 0x7fe66838c820>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e4780-3aeb-4829-b9ba-1faab5c8eb00",
   "metadata": {},
   "source": [
    "On v√©rifie un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3c023623-8dc5-49d5-89fe-c632d1447463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'bbox', 'labels', 'image'])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4774f41-f79e-4af4-8428-3f0b5f557671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
